{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02d928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m質問: グリニッジ標準時とUTCは何が違うのでしょうか?\u001b[0m\n",
      "---------------------------\n",
      "\u001b[34mグリニッジ標準時（GMT）と協定世界時（UTC）は、実際にはほとんど同じように使われることが多いけど、ちょっとした違いがあるんだわ。\n",
      "\n",
      "まず、GMTは、イギリスのグリニッジ天文台を基準にした時間で、主に天文学や航海のために使われてたんよ。一方、UTCは、国際的な標準時で、原子時計を基準にしてるから、より精密なんだわ。\n",
      "\n",
      "さらに、GMTは時間帯の一つとして使われることが多いけど、UTCは時間の標準として使われるから、例えば、UTC+9とかのように、他の時間帯との比較で使われることが多いわけさ。\n",
      "\n",
      "まとめると、GMTは歴史的な基準で、UTCは現代の標準って感じやね。\u001b[0m\n",
      "===========================\n",
      "\u001b[32m質問: なるほどです、では現在時刻とグリニッジ標準時を教えていただけますか?\u001b[0m\n",
      "---------------------------\n",
      "\u001b[34m\u001b[33m外部ツール get_now が呼び出されました ::: 現在時刻は 2026-26-02 16:02:31 です。 引数として utc=False を指定しました。\u001b[0m\n",
      "\u001b[33m外部ツール get_now が呼び出されました ::: 現在時刻は 2026-26-02 07:02:31 です。 引数として utc=True を指定しました。\u001b[0m\n",
      "\u001b[1;35m◆◆◆ 以下の回答は外部ツール get_now から得られた答えを使っています ◆◆◆\u001b[0m\n",
      "\u001b[35m今の時刻は、名古屋での時間が「2026年2月26日 16時02分31秒」やわ。グリニッジ標準時（GMT）は「2026年2月26日 07時02分31秒」やで。時間の差があるから、注意せんといかんね！\u001b[0m\n",
      "===========================\n",
      "ありがとうございました!\n"
     ]
    }
   ],
   "source": [
    "# 必要なモジュールをインポート\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.tools import tool\n",
    "\n",
    "# 別途必要なモジュール\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from langchain_core.messages import ToolMessage, AIMessageChunk\n",
    "\n",
    "\n",
    "# ===== Stateクラスの定義 =====\n",
    "# langchain では LLM に型を伝えるために typing で Annotation するのがミソ\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# ===== グラフの構築 =====\n",
    "def build_graph(model_name: str):\n",
    "    # 実質的には ChatOpenAI(model_name=...) と同等です\n",
    "    llm = LangchainAIClientGenerator.generateChatOpenAI(model_name=model_name)\n",
    "\n",
    "    # 外部ツールを定義して追加\n",
    "    tools = []\n",
    "    tools.append(UserDefTool.get_tool())\n",
    "    \n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # すみません、Tavily のアカウント取得ができなかったため\n",
    "    # 実際に以下の Tavily 外部ツールは試せていません\n",
    "    # 以下のようにすれば動くはずですが、もしも動作しなければご報告をいただけますでしょうか?\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "    tools.append(TavilySearchResults(max_results=2))\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    # 外部ツールを LLM にバインド\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    tool_node = ToolNode(tools)\n",
    "\n",
    "    # グラフの作成\n",
    "    graph_builder = StateGraph(State)\n",
    "\n",
    "    # グラフのノードを作成\n",
    "    node = ChatBotNode(llm_with_tools)\n",
    "    graph_builder.add_node(node.get_name(), node.get_node()) # チャットボットの追加\n",
    "    graph_builder.add_node(\"tools\", tool_node)               # ツールの追加\n",
    "\n",
    "    # ------------------\n",
    "    # ノードの結線\n",
    "    # ------------------\n",
    "    # 条件付エッジの作成\n",
    "    # チャットボット --(必要なら)--> ツールへ進むステップを追加\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"chatbot\",\n",
    "        tools_condition, # ツール呼出と判断したらツールノードたち (=tools) を呼ぶ\n",
    "    )\n",
    "\n",
    "    # ツール ---> チャットボットへ戻るステップを追加\n",
    "    graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "    # 開始ノードの指定\n",
    "    graph_builder.set_entry_point(node.get_name())\n",
    "\n",
    "    # 終了ノードの指定 (ループするので不要?)\n",
    "    # graph_builder.set_finish_point(node.get_name())\n",
    "\n",
    "    # ------------------\n",
    "    # グラフのビルド\n",
    "    # ------------------\n",
    "    # やり取りを記憶してもらう\n",
    "    memory = MemorySaver()\n",
    "\n",
    "    # 実行可能なステートグラフの作成\n",
    "    graph = graph_builder.compile(checkpointer=memory)\n",
    "    \n",
    "    # 可視化もできる\n",
    "    # from IPython.display import Image, display\n",
    "    # display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "    # グラフを返す\n",
    "    return graph\n",
    "\n",
    "\n",
    "# ===== グラフ実行関数 =====\n",
    "def stream_graph_updates(graph: StateGraph, user_input: str):\n",
    "    # Prompt (state) をつくる\n",
    "    messages = {\n",
    "        \"messages\": [\n",
    "            (\"system\", \"回答は名古屋弁でお願いします\"),\n",
    "            (\"user\", user_input)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Memo. もしも streaming しなくてよいならば以下で表示可能\n",
    "    # response = graph.invoke(\n",
    "    #     messages,\n",
    "    #     {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    # )\n",
    "    # for message in response[\"messages\"]:\n",
    "    #     print(message)\n",
    "    #     print(\"==================\")\n",
    "\n",
    "\n",
    "    # やりとりを記憶しておくために configurable を指定し stream_mode を設定する\n",
    "    # values   ... すべてのメッセージの履歴をストアしてくれる\n",
    "    # updates  ... ストアするのは前回取得との差分のみ\n",
    "    # messages ... LLM の回答のみ\n",
    "    events = graph.stream(\n",
    "        messages,\n",
    "        {\"configurable\": {\"thread_id\": \"1\"}},\n",
    "        stream_mode=\"messages\"\n",
    "    )\n",
    "\n",
    "    # 結果をストリーミングで得る (+表示)\n",
    "    print(\"\\033[32m\" + f\"質問: {user_input}\" + \"\\033[0m\")\n",
    "    print(\"---------------------------\")\n",
    "    \n",
    "    used_tools = []\n",
    "    llm_color  = \"\\033[34m\"\n",
    "\n",
    "    for event in events:        \n",
    "        chunk = event[0]\n",
    "\n",
    "        # ツールを使ったときの回答 (非同期だとどうなるんでしょう?)\n",
    "        if isinstance(chunk, ToolMessage):\n",
    "            print(\"\\033[33m\" + f\"外部ツール {chunk.name} が呼び出されました ::: {chunk.content}\" + \"\\033[0m\")\n",
    "            if chunk.name not in used_tools:\n",
    "                used_tools.append(chunk.name)\n",
    "                llm_color  = \"\\033[35m\"\n",
    "    \n",
    "        # LLM からの回答をリアルタイムで表示する\n",
    "        elif isinstance(chunk, AIMessageChunk):\n",
    "            if llm_color:\n",
    "                # 外部ツールを使っているならはじめに報告する\n",
    "                if len(used_tools) > 0:\n",
    "                    print(\"\\033[1;35m\" + f\"◆◆◆ 以下の回答は外部ツール {\", \".join(used_tools)} から得られた答えを使っています ◆◆◆\" + \"\\033[0m\")\n",
    "                # LLM の回答の色をセット\n",
    "                print(llm_color, end=\"\", flush=True)\n",
    "                llm_color = None\n",
    "    \n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "    # 色のリセット・改行\n",
    "    print(\"\\n\".join([ \"\\033[0m\", \"===========================\"]))\n",
    "\n",
    "\n",
    "\n",
    "# ===== メイン実行ロジック =====\n",
    "def main():\n",
    "    # モデル名\n",
    "    MODEL_NAME = \"gpt-4o-mini\" \n",
    "\n",
    "    # 環境変数の読み込み\n",
    "    load_dotenv(\"../.env\")\n",
    "    os.environ['OPENAI_API_KEY'] = os.environ['API_KEY']\n",
    "\n",
    "    # グラフの構築\n",
    "    graph = build_graph(MODEL_NAME)\n",
    "\n",
    "    # メインループ\n",
    "    while (user_input := input(\"質問:\")) != \"\":\n",
    "        stream_graph_updates(graph, user_input)\n",
    "\n",
    "    print(\"ありがとうございました!\")\n",
    "\n",
    "\n",
    "### =======================================\n",
    "### 以下、ユーティリティクラスとなります\n",
    "### =======================================\n",
    "\n",
    "# ChatOpenAI のインスタンスを入手するためのクラス\n",
    "class LangchainAIClientGenerator:\n",
    "\n",
    "    OPEN_API_ENV_NAME = \"API_KEY\"\n",
    "    \n",
    "    # この関数を呼び出すことで .env ファイルもしくは環境変数から OpenAI API キーをロードして\n",
    "    # ChatOpenAI の client インスタンスを返します\n",
    "    # api_key_validation      ... True なら簡易的なキーのチェックを行います\n",
    "    # display_debug_messages  ... True ならデバッグメッセージを表示します\n",
    "    # **kwargs                ... (model_name) モデルの指定などの langchain_openai.ChatOpenAI に渡すパラメータ\n",
    "    @classmethod\n",
    "    def generateChatOpenAI(cls, **kwargs):\n",
    "        kwargs[\"api_key\"] = cls.__load_api_key()\n",
    "        return ChatOpenAI(**kwargs)\n",
    "    \n",
    "\n",
    "    # この関数で OpenAI API のキーを読み取ります\n",
    "    # 1. 実行パスの ../.env が存在するならば API_KEY を環境変数としてロードして読み取る\n",
    "    # 2. 環境変数の API_KEY をロードして読み取る\n",
    "    # 3. API_KEY の中身の値が絶対ファイルパスならばその中身をそのままロードする\n",
    "    #    そうでないならば中身をそのまま API キーとして使う\n",
    "    # 4. キーの簡易的なフォーマットチェック\n",
    "    @classmethod\n",
    "    def __load_api_key(cls):        \n",
    "        api_key = None\n",
    "\n",
    "        # 1. ../.env を読み取る\n",
    "        env_file_path = Path().resolve().parent.resolve() / \".env\"\n",
    "        if env_file_path.is_file():\n",
    "            try:\n",
    "                load_dotenv(env_file_path)\n",
    "            except:\n",
    "                raise Exception(\"Found .env file but failed to load dotenv file! Please install python-dotenv module.\")\n",
    "\n",
    "        \n",
    "        # 2. API_KEY を読み取る\n",
    "        api_key = os.environ.get(cls.OPEN_API_ENV_NAME, None)\n",
    "\n",
    "        # 3. API_KEY の中身チェック\n",
    "        api_file_path = Path(api_key).expanduser()\n",
    "        if (api_file_path.is_absolute() and api_file_path.is_file()):\n",
    "            with open(api_file_path, \"r\") as f:\n",
    "                api_key = f.read().strip()\n",
    "\n",
    "        # 4. キーの簡易チェック\n",
    "        if re.match(r\"^sk\\-.*$\", api_key) is None:\n",
    "            raise Exception(\"Failed to load api key!\")\n",
    "        \n",
    "        return api_key\n",
    "\n",
    "\n",
    "# ChatBot 用のクラス、ノードの役割を果たします\n",
    "class ChatBotNode:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.name = \"chatbot\"\n",
    "\n",
    "\n",
    "    def chatbot(self, state):\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                self.llm.invoke(state[\"messages\"])\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def get_node(self):\n",
    "        return self.chatbot\n",
    "\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "\n",
    "# もしも外部ツールを自作したい場合について以下のように @tool アノテーションを使って関数を引き渡す??\n",
    "# Memo. もしも自作のツールを作って連動したい場合は以下のようにツールを作ることができる\n",
    "class UserDefTool:\n",
    "    # Langchain ではアノテーション (****:int) 部分で LLM に型を渡すので必須\n",
    "    # ※ そうでなくても型の明示的な宣言は良い習慣\n",
    "    @tool\n",
    "    @staticmethod\n",
    "    def get_now(is_utc : bool = False) -> str:\n",
    "        \"\"\"現在時刻を返します\n",
    "        Args:\n",
    "            is_utc: もしも協定世界時(UTC)もしくはグリニッジ標準時(GMT)を指定された場合は true としてください、そうでなければ false を代入してください\n",
    "        \"\"\"\n",
    "        # 上のドキュメントも必要です (ただし LLM にわたるので関数のドキュメント化とは少し違います)\n",
    "\n",
    "        from datetime import datetime, timezone\n",
    "        now = datetime.now(timezone.utc) if is_utc else datetime.now()\n",
    "        now_str = now.strftime(\"%Y-%d-%m %H:%M%:%S\")\n",
    "        response = f\"現在時刻は {now_str} です。 引数として utc={is_utc} を指定しました。\"\n",
    "        return response\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def get_tool(cls):\n",
    "        return cls.get_now\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
